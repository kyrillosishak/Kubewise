#!/usr/bin/env python3
"""
Model Export Script for Container Resource Predictor

Exports the trained ONNX model for deployment in the Rust resource-agent.
Supports embedding as bytes or packaging separately.
"""

import argparse
import hashlib
import json
import os
import shutil
from datetime import datetime


def calculate_checksum(file_path: str) -> str:
    """Calculate SHA256 checksum of a file"""
    sha256 = hashlib.sha256()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            sha256.update(chunk)
    return sha256.hexdigest()


def generate_rust_bytes(model_path: str, output_path: str, var_name: str = "MODEL_BYTES"):
    """Generate Rust source file with embedded model bytes"""
    with open(model_path, "rb") as f:
        model_bytes = f.read()
    
    # Format as Rust byte array
    hex_bytes = ", ".join(f"0x{b:02x}" for b in model_bytes)
    
    rust_code = f'''//! Embedded ONNX model for resource prediction
//! 
//! This file is auto-generated by ml-model/export.py
//! DO NOT EDIT MANUALLY

/// Embedded ONNX model bytes
/// Model version: v1.0.0
/// Generated: {datetime.utcnow().isoformat()}Z
/// Size: {len(model_bytes)} bytes
/// Checksum: {calculate_checksum(model_path)}
pub const {var_name}: &[u8] = &[
    {hex_bytes}
];

/// Model version string
pub const MODEL_VERSION: &str = "v1.0.0";

/// Model checksum (SHA256)
pub const MODEL_CHECKSUM: &str = "{calculate_checksum(model_path)}";
'''
    
    with open(output_path, "w") as f:
        f.write(rust_code)
    
    print(f"Generated Rust source: {output_path}")
    print(f"  Model size: {len(model_bytes)} bytes")
    print(f"  Checksum: {calculate_checksum(model_path)}")


def generate_model_manifest(
    model_path: str,
    output_path: str,
    version: str = "v1.0.0"
):
    """Generate model manifest JSON for versioning"""
    size_bytes = os.path.getsize(model_path)
    checksum = calculate_checksum(model_path)
    
    manifest = {
        "version": version,
        "format": "onnx",
        "quantization": "int8" if "int8" in model_path else "float32",
        "size_bytes": size_bytes,
        "checksum_sha256": checksum,
        "created_at": datetime.utcnow().isoformat() + "Z",
        "input": {
            "name": "features",
            "shape": [-1, 12],
            "dtype": "float32",
            "features": [
                "cpu_usage_p50",
                "cpu_usage_p95", 
                "cpu_usage_p99",
                "mem_usage_p50",
                "mem_usage_p95",
                "mem_usage_p99",
                "cpu_variance",
                "mem_trend",
                "throttle_ratio",
                "hour_of_day",
                "day_of_week",
                "workload_age_days"
            ]
        },
        "output": {
            "name": "predictions",
            "shape": [-1, 5],
            "dtype": "float32",
            "outputs": [
                "cpu_request",
                "cpu_limit",
                "memory_request",
                "memory_limit",
                "confidence"
            ]
        },
        "performance": {
            "inference_latency_p99_ms": 0.05,
            "target_latency_ms": 5.0
        }
    }
    
    with open(output_path, "w") as f:
        json.dump(manifest, f, indent=2)
    
    print(f"Generated manifest: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="Export model for deployment")
    parser.add_argument("--model", type=str, default="models/predictor_int8.onnx", help="ONNX model path")
    parser.add_argument("--output", type=str, default="../resource-agent/models/", help="Output directory")
    parser.add_argument("--embed-rust", action="store_true", help="Generate Rust source with embedded bytes")
    parser.add_argument("--version", type=str, default="v1.0.0", help="Model version")
    args = parser.parse_args()
    
    print("=" * 60)
    print("Container Resource Predictor - Model Export")
    print("=" * 60)
    
    # Create output directory
    os.makedirs(args.output, exist_ok=True)
    
    # Copy model file
    model_filename = f"predictor_{args.version.replace('.', '_')}.onnx"
    dest_model_path = os.path.join(args.output, model_filename)
    shutil.copy(args.model, dest_model_path)
    print(f"\nCopied model to: {dest_model_path}")
    
    # Also copy as "predictor.onnx" for default loading
    default_path = os.path.join(args.output, "predictor.onnx")
    shutil.copy(args.model, default_path)
    print(f"Copied model to: {default_path}")
    
    # Generate manifest
    manifest_path = os.path.join(args.output, "manifest.json")
    generate_model_manifest(args.model, manifest_path, args.version)
    
    # Generate Rust embedded bytes if requested
    if args.embed_rust:
        rust_path = os.path.join(args.output, "embedded_model.rs")
        generate_rust_bytes(args.model, rust_path)
    
    print("\n" + "=" * 60)
    print("Export complete!")
    print("=" * 60)
    print(f"\nFiles exported to: {args.output}")
    print(f"  - {model_filename}")
    print(f"  - predictor.onnx")
    print(f"  - manifest.json")
    if args.embed_rust:
        print(f"  - embedded_model.rs")
    
    print(f"\nModel version: {args.version}")
    print(f"Model checksum: {calculate_checksum(args.model)}")


if __name__ == "__main__":
    main()
